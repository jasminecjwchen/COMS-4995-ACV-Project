{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a2c911-3409-4ed1-aa61-f0307f6ca5a5",
   "metadata": {},
   "source": [
    "# Facial Anti-Spoofing using Deep Neural Network Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2177bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceead877-7fc0-4e9e-8d9c-ddb23fc46791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735fffe-eb8b-4662-a7dd-2437874453fc",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c17badd7-7e1f-42fa-9aa7-2d8dbdb55981",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/jasminecjwchen/Documents/GitHub/COMS-4995-ACV-Project/preprocessed_data\"\n",
    "test_dir = \"/Users/jasminecjwchen/Documents/GitHub/COMS-4995-ACV-Project/unseen_data\"\n",
    "output_dir = '/Users/jasminecjwchen/Documents/GitHub/COMS-4995-ACV-Project/split_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b94d9ac7-0b27-4ab5-badd-af0d236e7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(data_dir=data_dir, test_dir = test_dir, output_dir=output_dir):\n",
    "    \"\"\"\n",
    "    Create data splits for training, validation, and testing.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dir: Path to the directory containing the 'live' and 'spoof' subdirectories.\n",
    "    - output_dir: Path to the directory where the splits will be saved.\n",
    "    - split_ratio: A tuple indicating the split ratio for training, validation, and testing sets.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        os.makedirs(os.path.join(output_dir, 'train', 'live'))\n",
    "        os.makedirs(os.path.join(output_dir, 'train', 'spoof'))\n",
    "        os.makedirs(os.path.join(output_dir, 'val', 'live'))\n",
    "        os.makedirs(os.path.join(output_dir, 'val', 'spoof'))\n",
    "        os.makedirs(os.path.join(output_dir, 'test', 'live'))\n",
    "        os.makedirs(os.path.join(output_dir, 'test', 'spoof'))\n",
    "\n",
    "    for category in ['live', 'spoof']:\n",
    "        files = os.listdir(os.path.join(data_dir, category))\n",
    "        train_files, val_files = train_test_split(files, test_size=0.2, random_state=42)\n",
    "        test_files = os.listdir(os.path.join(test_dir, category))\n",
    "\n",
    "        for file in train_files:\n",
    "            shutil.copy(os.path.join(data_dir, category, file), os.path.join(output_dir, 'train', category, file))\n",
    "        for file in val_files:\n",
    "            shutil.copy(os.path.join(data_dir, category, file), os.path.join(output_dir, 'val', category, file))\n",
    "        for file in test_files:\n",
    "            shutil.copy(os.path.join(data_dir, category, file), os.path.join(output_dir, 'test', category, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d882a9b7-c90f-4b4d-b99e-221f5af32cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this copies all the images to a new folder so be cautious...! :(\n",
    "create_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5dc561-f916-4ea2-9723-ebe8f9aa1a04",
   "metadata": {},
   "source": [
    "## Preparing Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d06a0e-8b16-4ca4-bc2a-d317e718292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(output_dir, 'train'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=os.path.join(output_dir, 'val'), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(output_dir, 'test'), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68beac4e-72f1-4f31-b3eb-41140b9c74b6",
   "metadata": {},
   "source": [
    "## Defining the basic CNN Model -- First Try, Optimizing for Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01004cad-5e8a-4fa8-850a-2044d608e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 512)  \n",
    "        self.fc2 = nn.Linear(512, 2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d7dd6-4043-40ae-85d5-9743a46f276f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d7ad2bd-999e-4bc0-9b52-4cf3e6f9a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs = 10\n",
    "patience = 2 \n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0 \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # should experiment with other loss functions...\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # experiment with learning rate and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db879dd5-5b73-42a7-ae4d-1989bda7301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device...\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0deda34-02a7-42d8-8cb4-22285c98debb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.2948, Validation Loss: 0.0875, Recall: 0.9843, Precision: 0.9570, F1 Score: 0.9705, ROC-AUC: 0.9944\n",
      "Epoch 2/10, Training Loss: 0.0535, Validation Loss: 0.0711, Recall: 0.9903, Precision: 0.9629, F1 Score: 0.9764, ROC-AUC: 0.9963\n",
      "Epoch 3/10, Training Loss: 0.0255, Validation Loss: 0.0816, Recall: 0.9665, Precision: 0.9833, F1 Score: 0.9749, ROC-AUC: 0.9968\n",
      "Epoch 4/10, Training Loss: 0.0173, Validation Loss: 0.0992, Recall: 0.9689, Precision: 0.9757, F1 Score: 0.9723, ROC-AUC: 0.9963\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    pred_probs = []  # Store predicted probabilities for AUC computation\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            probs = softmax(outputs, dim=1) # softmax for ROC-AUC\n",
    "            pred_probs.extend(probs[:, 1].cpu().numpy())  \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(preds.cpu().numpy())\n",
    "    \n",
    "    val_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_recall = recall_score(true_labels, pred_labels, average='binary')\n",
    "    val_precision = precision_score(true_labels, pred_labels, average='binary')\n",
    "    val_f1 = f1_score(true_labels, pred_labels, average='binary')\n",
    "    val_roc_auc = roc_auc_score(true_labels, pred_probs)  \n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}, F1 Score: {val_f1:.4f}, ROC-AUC: {val_roc_auc:.4f}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40f4e10c-364f-4789-9987-ad4d42c7f481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=200704, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNModel()\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab230958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9722315155570425\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    running_corrects = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    print(\"test accuracy\", (running_corrects.double() / len(test_dataset)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b7ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
